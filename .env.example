# .env.example - Example environment variables for Research Compass
# Copy this file to `.env` and fill in any secret values. Do NOT commit
# your real `.env` with credentials or API keys.

# ============================================================================
# System Configuration
# ============================================================================
ENV=development
LOG_LEVEL=INFO
DEBUG=false
AUTO_RELOAD=false
MAX_WORKERS=4
REQUEST_TIMEOUT=60.0

# ============================================================================
# Database Configuration
# ============================================================================
NEO4J_URI=neo4j://127.0.0.1:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here
NEO4J_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30.0
NEO4J_MAX_CONNECTION_LIFETIME=3600.0

# ============================================================================
# LLM Configuration - Unified Multi-Provider Support
# ============================================================================

# LLM Provider Settings
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1000
LLM_TIMEOUT=30.0
LLM_MAX_RETRIES=2

# Provider-Specific Settings
OLLAMA_BASE_URL=http://localhost:11434
LMSTUDIO_BASE_URL=http://localhost:1234
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Legacy Support (Deprecated - use LLM_PROVIDER instead)
USE_OLLAMA=True
USE_OPENAI=False
OLLAMA_MODEL=deepseek-r1:1.5b
OPENAI_MODEL=gpt-4o-mini

# ============================================================================
# Embedding Model Configuration
# ============================================================================
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
EMBEDDING_PROVIDER=huggingface
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_DIMENSION=384
EMBEDDING_BATCH_SIZE=32

# ============================================================================
# Vector store / Pinecone Configuration
# ============================================================================
# Choose vector backend: faiss, pinecone, chroma
VECTOR_PROVIDER=faiss

# Pinecone (cloud) / Pinecone Lite (local) settings
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=gcp-starter
PINECONE_INDEX_NAME=research-compass
PINECONE_DIMENSION=384
PINECONE_METRIC=cosine
PINECONE_USE_LOCAL=false  # set true to use Pinecone Lite or local dev server

# FAISS (local) settings
FAISS_INDEX_DIR=./data/faiss

# Chroma settings (if using CHROMA provider)
CHROMA_DIR=./data/chroma


# ============================================================================
# Document Processing Configuration
# ============================================================================
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_CHUNKS=5
MAX_GRAPH_DEPTH=3
USE_LLAMA_INDEX=true
CHUNK_STRATEGY=hybrid
MAX_FILE_SIZE=52428800  # 50MB

# Metadata Extraction
METADATA_EXTRACTION_USE_LLM_FALLBACK=true
METADATA_EXTRACTION_CONFIDENCE_THRESHOLD=0.7

# ============================================================================
# Academic Configuration
# ============================================================================

# GNN Settings
GNN_TRAIN_ON_STARTUP=false
GNN_MODEL_CHECKPOINT_DIR=models/gnn
GNN_NODE_CLASSIFIER_HIDDEN_DIM=256
GNN_NODE_CLASSIFIER_NUM_LAYERS=3
GNN_NODE_CLASSIFIER_DROPOUT=0.5

# Indexing Settings
INDEXING_USE_LLAMA_INDEX=true
INDEXING_CHUNK_STRATEGY=hybrid
INDEXING_CHUNK_SIZE=512
INDEXING_CHUNK_OVERLAP=50

# Recommendation Settings
RECOMMENDATIONS_TOP_K=20
RECOMMENDATIONS_MIN_CONFIDENCE=0.6
RECOMMENDATIONS_DIVERSITY_WEIGHT=0.3

# Citation Analysis Settings
CITATION_ANALYSIS_MAX_DEPTH=3
CITATION_ANALYSIS_MIN_CITATIONS=5

# ============================================================================
# UI Configuration
# ============================================================================
GRADIO_PORT=7860
GRADIO_HOST=0.0.0.0
GRADIO_SHARE=false
GRADIO_SERVER_NAME=127.0.0.1
GRADIO_PREFERRED_PORT=7860

# Visualization Settings
VIZ_HEIGHT=800px
VIZ_WIDTH=100%
VIZ_BG_COLOR=#1a1a1a
VIZ_FONT_COLOR=white
DEFAULT_MAX_NODES=200
DEFAULT_LAYOUT=force_directed
DEFAULT_NODE_SIZING=degree

# ============================================================================
# Cache Configuration
# ============================================================================
CACHE_ENABLED=true
CACHE_DIR=data/cache
CACHE_MAX_ITEMS=1000
CACHE_DEFAULT_TTL=3600
CACHE_CLEANUP_INTERVAL=300

# ============================================================================
# Path Configuration
# ============================================================================
DATA_DIR=data
DOCUMENTS_DIR=data/documents
INDICES_DIR=data/indices
CACHE_DIR=data/cache
OUTPUT_DIR=output
VISUALIZATION_DIR=output/visualizations
REPORTS_DIR=output/reports
EXPORTS_DIR=output/exports
MODELS_DIR=models
GNN_MODELS_DIR=models/gnn
CHROMA_DIR=./data/chroma

# ============================================================================
# Enhanced Features Configuration
# ============================================================================

# Version Management
VERSION_SNAPSHOT_ENABLED=true
MAX_VERSIONS=50

# Temporal Queries
TEMPORAL_INDEX_ENABLED=true
DEFAULT_TIME_WINDOW_HOURS=24

# ============================================================================
# Legacy Environment Variables (Deprecated)
# ============================================================================
# These variables are maintained for backward compatibility only.
# Use the unified LLM_PROVIDER configuration instead.

# Legacy GNN Settings (Deprecated - use NEO4J_* settings instead)
ENABLE_GNN=false
GNN_URI=bolt://127.0.0.1:7687
GNN_USER=neo4j
GNN_PASSWORD=changeme

# Legacy Path Settings (Deprecated - use PATH_* settings instead)
GRADIO_DATA_DIR=graphrag_data
GRADIO_OUTPUT_DIR=output
GRADIO_EXPORT_DIR=output/exports

# Note: After copying to .env, set all password fields to secure values
# and keep .env out of version control.
