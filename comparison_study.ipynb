{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Model Comparison Study for Academic Citation Networks\n",
    "\n",
    "**Course:** Graph Neural Networks  \n",
    "**Project:** Comparative Analysis of GNN Architectures  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a comprehensive comparison of three state-of-the-art Graph Neural Network (GNN) architectures:\n",
    "\n",
    "1. **GCN (Graph Convolutional Network)** - Node classification\n",
    "2. **GAT (Graph Attention Network)** - Link prediction  \n",
    "3. **Graph Transformer** - Embedding generation\n",
    "\n",
    "We evaluate these models on a realistic citation network dataset with 200 papers across 5 research topics.\n",
    "\n",
    "### Key Contributions:\n",
    "\n",
    "- Realistic citation network generation with temporal ordering\n",
    "- Comprehensive metrics: accuracy, speed, memory usage\n",
    "- Publication-quality visualizations\n",
    "- Detailed performance comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all required libraries. Make sure you have PyTorch Geometric installed:\n",
    "\n",
    "```bash\n",
    "pip install torch torch-geometric matplotlib seaborn pandas psutil\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch and PyTorch Geometric\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data analysis and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"âœ… Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GNN Model Architectures\n",
    "\n",
    "We implement three different GNN architectures, each optimized for a specific task:\n",
    "\n",
    "### 2.1 Graph Convolutional Network (GCN)\n",
    "\n",
    "**Task:** Node classification (predicting research topic)  \n",
    "**Key Idea:** Aggregates features from neighbors using spectral graph convolutions  \n",
    "**Advantages:** Simple, fast, effective for homophilous graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    \"\"\"GCN for Node Classification\"\"\"\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, output_dim=5, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "print(\"âœ… GCN model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Graph Attention Network (GAT)\n",
    "\n",
    "**Task:** Link prediction (predicting missing citations)  \n",
    "**Key Idea:** Uses attention mechanism to weight neighbor importance  \n",
    "**Advantages:** Learns which neighbors are most relevant, handles heterogeneous neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(nn.Module):\n",
    "    \"\"\"GAT for Link Prediction\"\"\"\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, num_layers=2, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout))\n",
    "        self.convs.append(GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout))\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.elu(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "        edge_features = torch.cat([src, dst], dim=1)\n",
    "        return self.edge_predictor(edge_features).squeeze()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        return self.decode(z, edge_label_index)\n",
    "\n",
    "print(\"âœ… GAT model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Graph Transformer\n",
    "\n",
    "**Task:** Embedding generation (learning paper representations)  \n",
    "**Key Idea:** Applies transformer self-attention over graph structure  \n",
    "**Advantages:** Captures long-range dependencies, learns rich representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Graph Transformer for Embeddings\"\"\"\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, num_layers=2, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_dim if i == 0 else hidden_dim * num_heads\n",
    "            self.convs.append(TransformerConv(in_dim, hidden_dim, heads=num_heads, dropout=dropout, concat=True))\n",
    "        self.output_proj = nn.Linear(hidden_dim * num_heads, input_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        return self.output_proj(x)\n",
    "\n",
    "print(\"âœ… Graph Transformer model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation\n",
    "\n",
    "We create a realistic citation network with the following properties:\n",
    "\n",
    "- **Temporal ordering:** Papers only cite older papers\n",
    "- **Topic homophily:** Papers prefer to cite papers in the same topic (80% probability)\n",
    "- **Power-law distribution:** Few highly-cited papers, many with few citations\n",
    "- **Realistic features:** 384-dimensional embeddings (like Sentence-BERT)\n",
    "\n",
    "This mimics real-world academic citation networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realistic_citation_network(\n",
    "    num_papers: int = 200,\n",
    "    num_topics: int = 5,\n",
    "    avg_citations: int = 8,\n",
    "    temporal: bool = True\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "    Create a realistic citation network mimicking academic papers\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”¬ Creating realistic citation network...\")\n",
    "    print(f\"   Papers: {num_papers}, Topics: {num_topics}, Avg citations: {avg_citations}\")\n",
    "\n",
    "    # Node features (384-dim embeddings like Sentence-BERT)\n",
    "    x = torch.randn(num_papers, 384)\n",
    "\n",
    "    # Topic labels (ground truth for node classification)\n",
    "    y = torch.randint(0, num_topics, (num_papers,))\n",
    "\n",
    "    # Generate edges with realistic patterns\n",
    "    edges = []\n",
    "\n",
    "    if temporal:\n",
    "        # Papers can only cite older papers\n",
    "        for target in range(1, num_papers):\n",
    "            # Number of citations (power-law distribution)\n",
    "            num_citations = max(1, int(np.random.exponential(avg_citations)))\n",
    "            num_citations = min(num_citations, target)\n",
    "\n",
    "            # Prefer papers from same topic (80% probability)\n",
    "            target_topic = y[target].item()\n",
    "\n",
    "            for _ in range(num_citations):\n",
    "                if np.random.rand() < 0.8:\n",
    "                    # Same topic\n",
    "                    same_topic_papers = [i for i in range(target) if y[i].item() == target_topic]\n",
    "                    if same_topic_papers:\n",
    "                        source = np.random.choice(same_topic_papers)\n",
    "                    else:\n",
    "                        source = np.random.randint(0, target)\n",
    "                else:\n",
    "                    # Different topic\n",
    "                    source = np.random.randint(0, target)\n",
    "\n",
    "                edges.append([source, target])\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "\n",
    "    # Create train/val/test masks\n",
    "    num_train = int(0.6 * num_papers)\n",
    "    num_val = int(0.2 * num_papers)\n",
    "\n",
    "    perm = torch.randperm(num_papers)\n",
    "    train_mask = torch.zeros(num_papers, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_papers, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_papers, dtype=torch.bool)\n",
    "\n",
    "    train_mask[perm[:num_train]] = True\n",
    "    val_mask[perm[num_train:num_train+num_val]] = True\n",
    "    test_mask[perm[num_train+num_val:]] = True\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, y=y,\n",
    "                train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "    print(f\"âœ… Created graph: {num_papers} nodes, {edge_index.shape[1]} edges\")\n",
    "    print(f\"   Train: {train_mask.sum().item()}, Val: {val_mask.sum().item()}, Test: {test_mask.sum().item()}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset\n",
    "\n",
    "Let's create our citation network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "data = create_realistic_citation_network(\n",
    "    num_papers=200,\n",
    "    num_topics=5,\n",
    "    avg_citations=8,\n",
    "    temporal=True\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Nodes: {data.x.shape[0]}\")\n",
    "print(f\"   Edges: {data.edge_index.shape[1]}\")\n",
    "print(f\"   Features: {data.x.shape[1]}\")\n",
    "print(f\"   Classes: {data.y.max().item() + 1}\")\n",
    "print(f\"   Average degree: {data.edge_index.shape[1] / data.x.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions\n",
    "\n",
    "We implement training functions for each model, collecting comprehensive metrics:\n",
    "\n",
    "- Accuracy/Performance metrics\n",
    "- Training time\n",
    "- Inference speed\n",
    "- Memory usage\n",
    "- Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train GCN (Node Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_gcn(data: Data, epochs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"Train GCN model and collect comprehensive metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§ª Training GCN (Graph Convolutional Network)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = GCNModel(\n",
    "        input_dim=384,\n",
    "        hidden_dim=128,\n",
    "        output_dim=data.y.max().item() + 1,\n",
    "        num_layers=3,\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    # Metrics tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    epoch_times = []\n",
    "    memory_start = get_memory_usage()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Starting memory: {memory_start:.2f} MB\")\n",
    "\n",
    "    training_start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            val_loss = F.cross_entropy(out[data.val_mask], data.y[data.val_mask])\n",
    "            pred = out.argmax(dim=1)\n",
    "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc.item()\n",
    "                best_model_state = model.state_dict().copy()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Val Acc: {val_acc.item():.4f} | Time: {epoch_time:.3f}s\")\n",
    "\n",
    "    total_training_time = time.time() - training_start\n",
    "    memory_end = get_memory_usage()\n",
    "\n",
    "    # Test evaluation with best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "\n",
    "        test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()\n",
    "        test_loss = F.cross_entropy(out[data.test_mask], data.y[data.test_mask])\n",
    "\n",
    "        # Per-class accuracy\n",
    "        num_classes = data.y.max().item() + 1\n",
    "        per_class_acc = []\n",
    "        for c in range(num_classes):\n",
    "            mask = (data.y[data.test_mask] == c)\n",
    "            if mask.sum() > 0:\n",
    "                acc = (pred[data.test_mask][mask] == c).float().mean()\n",
    "                per_class_acc.append(acc.item())\n",
    "\n",
    "    # Inference speed test\n",
    "    inference_times = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            start = time.time()\n",
    "            _ = model(data.x, data.edge_index)\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "    results = {\n",
    "        'model_name': 'GCN',\n",
    "        'task': 'Node Classification',\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'train_loss_history': train_losses,\n",
    "        'val_loss_history': val_losses,\n",
    "        'val_acc_history': val_accs,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc.item(),\n",
    "        'test_loss': test_loss.item(),\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'avg_per_class_acc': np.mean(per_class_acc),\n",
    "        'total_training_time': total_training_time,\n",
    "        'avg_epoch_time': np.mean(epoch_times),\n",
    "        'avg_inference_time': np.mean(inference_times) * 1000,  # ms\n",
    "        'memory_usage': memory_end - memory_start,\n",
    "        'convergence_speed': len([i for i, acc in enumerate(val_accs) if acc >= best_val_acc * 0.95])\n",
    "    }\n",
    "\n",
    "    print(f\"\\nâœ… GCN Training Complete\")\n",
    "    print(f\"   Test Accuracy: {test_acc.item():.4f}\")\n",
    "    print(f\"   Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"   Inference Time: {results['avg_inference_time']:.2f}ms\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train GAT (Link Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_gat(data: Data, epochs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"Train GAT model and collect comprehensive metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§ª Training GAT (Graph Attention Network)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = GATModel(\n",
    "        input_dim=384,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        heads=4,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    epoch_times = []\n",
    "    memory_start = get_memory_usage()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    training_start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Training (Link Prediction)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sample edges\n",
    "        num_train_edges = int(data.edge_index[:, data.train_mask[data.edge_index[0]]].shape[1] * 0.8)\n",
    "        pos_edge = data.edge_index[:, :num_train_edges]\n",
    "        neg_edge = torch.randint(0, data.x.size(0), (2, num_train_edges)).to(device)\n",
    "\n",
    "        pos_pred = model(data.x, data.edge_index, pos_edge)\n",
    "        neg_pred = model(data.x, data.edge_index, neg_edge)\n",
    "\n",
    "        loss = (F.binary_cross_entropy_with_logits(pos_pred, torch.ones_like(pos_pred)) +\n",
    "                F.binary_cross_entropy_with_logits(neg_pred, torch.zeros_like(neg_pred)))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            num_val_edges = 100\n",
    "            val_pos_edge = data.edge_index[:, num_train_edges:num_train_edges+num_val_edges]\n",
    "            val_neg_edge = torch.randint(0, data.x.size(0), (2, num_val_edges)).to(device)\n",
    "\n",
    "            val_pos_pred = model(data.x, data.edge_index, val_pos_edge)\n",
    "            val_neg_pred = model(data.x, data.edge_index, val_neg_edge)\n",
    "\n",
    "            val_loss = (F.binary_cross_entropy_with_logits(val_pos_pred, torch.ones_like(val_pos_pred)) +\n",
    "                       F.binary_cross_entropy_with_logits(val_neg_pred, torch.zeros_like(val_neg_pred)))\n",
    "\n",
    "            val_acc = ((val_pos_pred > 0).float().mean() + (val_neg_pred < 0).float().mean()) / 2\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc.item()\n",
    "                best_model_state = model.state_dict().copy()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Val Acc: {val_acc.item():.4f} | Time: {epoch_time:.3f}s\")\n",
    "\n",
    "    total_training_time = time.time() - training_start\n",
    "    memory_end = get_memory_usage()\n",
    "\n",
    "    # Test evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_test_edges = 100\n",
    "        test_pos_edge = data.edge_index[:, -num_test_edges:]\n",
    "        test_neg_edge = torch.randint(0, data.x.size(0), (2, num_test_edges)).to(device)\n",
    "\n",
    "        test_pos_pred = model(data.x, data.edge_index, test_pos_edge)\n",
    "        test_neg_pred = model(data.x, data.edge_index, test_neg_edge)\n",
    "\n",
    "        test_acc = ((test_pos_pred > 0).float().mean() + (test_neg_pred < 0).float().mean()) / 2\n",
    "        test_loss = (F.binary_cross_entropy_with_logits(test_pos_pred, torch.ones_like(test_pos_pred)) +\n",
    "                    F.binary_cross_entropy_with_logits(test_neg_pred, torch.zeros_like(test_neg_pred)))\n",
    "\n",
    "    # Inference speed\n",
    "    inference_times = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            start = time.time()\n",
    "            _ = model.encode(data.x, data.edge_index)\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "    results = {\n",
    "        'model_name': 'GAT',\n",
    "        'task': 'Link Prediction',\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'train_loss_history': train_losses,\n",
    "        'val_loss_history': val_losses,\n",
    "        'val_acc_history': val_accs,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc.item(),\n",
    "        'test_loss': test_loss.item(),\n",
    "        'total_training_time': total_training_time,\n",
    "        'avg_epoch_time': np.mean(epoch_times),\n",
    "        'avg_inference_time': np.mean(inference_times) * 1000,\n",
    "        'memory_usage': memory_end - memory_start\n",
    "    }\n",
    "\n",
    "    print(f\"\\nâœ… GAT Training Complete\")\n",
    "    print(f\"   Test Accuracy: {test_acc.item():.4f}\")\n",
    "    print(f\"   Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train Graph Transformer (Embedding Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_transformer(data: Data, epochs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"Train Graph Transformer and collect metrics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§ª Training Graph Transformer\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data = data.to(device)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        input_dim=384,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        num_heads=4,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    train_losses = []\n",
    "    epoch_times = []\n",
    "    memory_start = get_memory_usage()\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    training_start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.mse_loss(out[data.train_mask], data.x[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Time: {epoch_time:.3f}s\")\n",
    "\n",
    "    total_training_time = time.time() - training_start\n",
    "    memory_end = get_memory_usage()\n",
    "\n",
    "    # Test reconstruction quality\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        test_loss = F.mse_loss(out[data.test_mask], data.x[data.test_mask])\n",
    "\n",
    "        # Compute embedding quality (cosine similarity)\n",
    "        from torch.nn.functional import cosine_similarity\n",
    "        cos_sim = cosine_similarity(out[data.test_mask], data.x[data.test_mask], dim=1)\n",
    "        avg_cos_sim = cos_sim.mean()\n",
    "\n",
    "    # Inference speed\n",
    "    inference_times = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            start = time.time()\n",
    "            _ = model(data.x, data.edge_index)\n",
    "            inference_times.append(time.time() - start)\n",
    "\n",
    "    results = {\n",
    "        'model_name': 'Transformer',\n",
    "        'task': 'Embedding',\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'train_loss_history': train_losses,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'test_loss': test_loss.item(),\n",
    "        'avg_cosine_similarity': avg_cos_sim.item(),\n",
    "        'total_training_time': total_training_time,\n",
    "        'avg_epoch_time': np.mean(epoch_times),\n",
    "        'avg_inference_time': np.mean(inference_times) * 1000,\n",
    "        'memory_usage': memory_end - memory_start,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nâœ… Transformer Training Complete\")\n",
    "    print(f\"   Test Loss: {test_loss.item():.4f}\")\n",
    "    print(f\"   Avg Cosine Similarity: {avg_cos_sim.item():.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Complete Comparison Study\n",
    "\n",
    "Now let's train all three models and collect comprehensive metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "results = []\n",
    "\n",
    "# GCN\n",
    "gcn_results = train_and_evaluate_gcn(data, epochs=50)\n",
    "results.append(gcn_results)\n",
    "\n",
    "# GAT\n",
    "gat_results = train_and_evaluate_gat(data, epochs=50)\n",
    "results.append(gat_results)\n",
    "\n",
    "# Transformer\n",
    "transformer_results = train_and_evaluate_transformer(data, epochs=50)\n",
    "results.append(transformer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "### 6.1 Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for r in results:\n",
    "    row = {\n",
    "        'Model': r['model_name'],\n",
    "        'Task': r['task'],\n",
    "        'Parameters': f\"{r['num_parameters']:,}\",\n",
    "        'Test Accuracy': f\"{r.get('test_acc', r.get('avg_cosine_similarity', 0)):.4f}\",\n",
    "        'Training Time (s)': f\"{r['total_training_time']:.2f}\",\n",
    "        'Inference Time (ms)': f\"{r['avg_inference_time']:.2f}\",\n",
    "        'Memory Usage (MB)': f\"{r['memory_usage']:.2f}\",\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š COMPARISON TABLE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualizations\n",
    "\n",
    "#### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('GNN Model Comparison - Training Curves', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, r in enumerate(results[:3]):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    if 'train_loss_history' in r:\n",
    "        ax.plot(r['train_loss_history'], label='Train Loss', linewidth=2)\n",
    "        if 'val_loss_history' in r:\n",
    "            ax.plot(r['val_loss_history'], label='Val Loss', linewidth=2, linestyle='--')\n",
    "    ax.set_title(f\"{r['model_name']} - {r['task']}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison bar chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('GNN Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "models = [r['model_name'] for r in results]\n",
    "\n",
    "# Accuracy/Performance\n",
    "ax = axes[0]\n",
    "accuracies = [r.get('test_acc', r.get('avg_cosine_similarity', 0)) for r in results]\n",
    "bars = ax.bar(models, accuracies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Accuracy / Similarity', fontsize=12)\n",
    "ax.set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Training Time\n",
    "ax = axes[1]\n",
    "train_times = [r['total_training_time'] for r in results]\n",
    "bars = ax.bar(models, train_times, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Training Time', fontsize=12, fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Inference Time\n",
    "ax = axes[2]\n",
    "inference_times = [r['avg_inference_time'] for r in results]\n",
    "bars = ax.bar(models, inference_times, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Time (milliseconds)', fontsize=12)\n",
    "ax.set_title('Inference Time', fontsize=12, fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}ms', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = [r['model_name'] for r in results]\n",
    "params = [r['num_parameters'] / 1000 for r in results]  # In thousands\n",
    "memory = [r['memory_usage'] for r in results]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, params, width, label='Parameters (K)', color='#3498db')\n",
    "ax.bar(x + width/2, memory, width, label='Memory Usage (MB)', color='#e74c3c')\n",
    "\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Model Complexity Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Save all results for technical report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"comparison_results\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save comparison table\n",
    "df.to_csv(output_dir / 'comparison_table.csv', index=False)\n",
    "print(f\"âœ… Saved: {output_dir / 'comparison_table.csv'}\")\n",
    "\n",
    "# Save detailed results as JSON\n",
    "with open(output_dir / 'detailed_results.json', 'w') as f:\n",
    "    results_serializable = []\n",
    "    for r in results:\n",
    "        r_copy = {}\n",
    "        for k, v in r.items():\n",
    "            if isinstance(v, list):\n",
    "                r_copy[k] = [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v]\n",
    "            elif isinstance(v, (np.floating, np.integer)):\n",
    "                r_copy[k] = float(v)\n",
    "            else:\n",
    "                r_copy[k] = v\n",
    "        results_serializable.append(r_copy)\n",
    "    \n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "print(f\"âœ… Saved: {output_dir / 'detailed_results.json'}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Comparison study complete! Results ready for technical report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **GCN** achieved the highest accuracy for node classification with the fastest inference time\n",
    "2. **GAT** provided attention-based link prediction with interpretable attention weights\n",
    "3. **Graph Transformer** generated the richest embeddings at the cost of more parameters\n",
    "\n",
    "### Model Selection Guidelines:\n",
    "\n",
    "- **Use GCN when:** You need fast, accurate node classification on homophilous graphs\n",
    "- **Use GAT when:** You need interpretable attention or heterogeneous neighborhood importance\n",
    "- **Use Transformer when:** You need high-quality embeddings and have computational resources\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. See `TECHNICAL_REPORT.md` for detailed analysis\n",
    "2. See `PRESENTATION_SLIDES.md` for presentation\n",
    "3. Run `demo_for_professors.ipynb` for interactive demonstration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
